{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWMGPce-Cu_J"
      },
      "source": [
        "# **CSE354 HW3**\n",
        "**Due date: 11:59 pm EST on April 24, 2022 (Sunday)**\n",
        "\n",
        "---\n",
        "For this assignment, we will use Google Colab, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You can use your Stony Brook (*.stonybrook.edu) account or your personal gmail account for coding and Google Drive to save your results.\n",
        "\n",
        "## **Google Colab Tutorial**\n",
        "---\n",
        "Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n",
        "\n",
        "**This notebook would need you to train your model on Colab's GPU. However, the runtimes are limited. So ensure that your code works on the default CPU runtime before switching over to the GPU runtime.**\n",
        "\n",
        "## **Problem statement**\n",
        "---\n",
        "In this homework, you will be using language models to predict the sentiment of a given movie review. The dataset, which is given to you, is sampled from the [IMDB dataset of 50k movie reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). The sentences are sampled to a smaller set to help with quicker computation on Colab. The data contains a review and an associated label for the sentiment of that review. The label can either be *positive* or *negative*. You have been given three files - train_data.csv, val_data.csv and test_data.csv. The training data will be used to fine-tune the language model, the val data will be used to select the best model while training and finally the test data will give the model's final performance on the data.\n",
        "\n",
        "To perform this task you will be using a pre-trained DistilBERT model. DistilBERT is a BERT based language model. Its size is 40% lesser than BERT, it has around 97% of BERT's language understanding capabilities and is 60% faster. You can read more about DistilBERT - https://arxiv.org/abs/1910.01108.\n",
        "\n",
        "You will be using the model by taking advantage of the libraries provided by Hugging Face (https://huggingface.co/). In order to use this library, it will need to be installed using the command in the cell below. You will be training four different DistilBERT models for this assignment.\n",
        "\n",
        "Fill in the # TODO(students) portions in this Colab file for this assignment.\n",
        "\n",
        "**Todos for the assignment:**\n",
        "*   Fill in the # TODO(students) portions in this Colab file for this assignment.\n",
        "*   Run the experiment code blocks and note down the colab outputs in a separate text file.\n",
        "*   Use the aformentioned colab outputs for writing the report as per submission guideline that is described at the end of this colab file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om1HnMs-Gw3c",
        "outputId": "ffb815a4-d6ae-44b8-ffe8-a314cc64b8f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 55.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=edd0caf3ee414a893cbbeb2035264e1d624df7cd003088e26e59ebc547b61a45\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6-oi7JhGjjB"
      },
      "source": [
        "## **Imports**\n",
        "---\n",
        "\n",
        "All the allowed imports have been done for you in the code block below. You do need and will not be allowed to use any more imports other than the ones done below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0N6kcCYG3tn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AdamW\n",
        "import os\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8rQ5vERGwes"
      },
      "source": [
        "## **Mounting your drive**\n",
        "---\n",
        "\n",
        "I would highly recommend mounting you Google Drive while running this notebook. This drive could contain the path to your dataset and it will also be used to save your fine-tuned models. In case you choose to simply save the models on your Colab workspace, the models will cease to exist after the runtime disconnects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIrvZGP08z-1",
        "outputId": "7cd76f49-8959-474c-e4dc-8f1a759a1cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFyqw_sxnEh8",
        "outputId": "6f697ecb-f7be-47eb-b7fc-90518b82f7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVvQyFjTm4hW",
        "outputId": "c38efcaf-ab8e-4bbb-ad1d-641d9978e45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HW3_Release\n",
            "'Copy of NLP_HW3.ipynb'   data\t models\n"
          ]
        }
      ],
      "source": [
        "#Set the path of the folder where your colab file and data exist in Google Drive in the ------ porition\n",
        "\n",
        "# TODO(students): start\n",
        "%cd \"drive/MyDrive/HW3_Release\"\n",
        "!ls\n",
        "# TODO(students): end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kihyF-BLHgou"
      },
      "source": [
        "## **Constants in the file**\n",
        "---\n",
        "\n",
        "The code block below contains a few constants.\n",
        "\n",
        "\n",
        "1.   **BATCH_SIZE**: The batch size input to the models. This has been set to 16 and should not be changed. In case you encounter any CUDA - out of memory errors while training your models, this value may be reduced from 16. But please mention this in your submission report.\n",
        "2.   **EPOCHS**: The number of epochs to train your model. This should not be changed.\n",
        "3. **TEST_PATH**: This is the path to the test_data.csv file.\n",
        "4. **TRAIN_PATH**: This is the path to the train_data.csv file.\n",
        "5. **VAL_PATH**: This is the path to the val_data.csv file.\n",
        "6. **SAVE_PATH**: This is the path to directory your model will be saved. Note: This path will be altered further down in the code by appending the name of the kind of DistilBERT model you train as per your experiments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrivEEHR0kUq"
      },
      "outputs": [],
      "source": [
        "#DO NOT CHANGE THE CONSTANTS\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "TEST_PATH = \"data/test_data.csv\"\n",
        "TRAIN_PATH = \"data/train_data.csv\"\n",
        "VAL_PATH = \"data/val_data.csv\"\n",
        "SAVE_PATH = \"models/DistilBERT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iuId22dbbSq"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path):\n",
        "  dataset = pd.read_csv(path)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQk4i6bH-LUo"
      },
      "outputs": [],
      "source": [
        "train_data = load_dataset(TRAIN_PATH)\n",
        "val_data = load_dataset(VAL_PATH)\n",
        "test_data = load_dataset(TEST_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz5glgyTJRkL"
      },
      "source": [
        "## **Problem 1 (Initialize the Model Class)**\n",
        "---\n",
        "\n",
        "Here, we will setup the pre-trained DistillBert model class in order to do our binary sentiment analysis task. In the code block below, you would need to load a pre-trained DistilBERT model and it's tokenizer using Hugging Face's library. The model you would need to load is called \"distilbert-base-uncased\". It would also need to have the model hyperparameter set to *num_classes* as the output shape of the model (in this case it is going to be 2, positive and negative). Please write your code between the given TODO block.\n",
        "\n",
        "\n",
        "\n",
        "More about the model and how to load it can be read at - https://huggingface.co/distilbert-base-uncased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmXIi5Y8fmcn"
      },
      "outputs": [],
      "source": [
        "class DistillBERT():\n",
        "\n",
        "  def __init__(self, model_name='distilbert-base-uncased', num_classes=2):\n",
        "    # TODO(students): start\n",
        "    self.num_classes = num_classes\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = self.num_classes)\n",
        "    \n",
        "    # TODO(students): end\n",
        "\n",
        "  def get_tokenizer_and_model(self):\n",
        "    return self.model, self.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZWzKZbiKMFP"
      },
      "source": [
        "## **Problem 2 (Initialize the Dataloader Class)**\n",
        "---\n",
        "Here, we will setup the dataloader class which will read data, tokenize it using the DistillBert tokenizer, converts the tokenized sentence to tensors and the labels to tensors. The code block below takes your dataset(train,validation or test) and the tokenizer you loaded in the previous block and generates the DataLoader object for it. You would need to implement a part of the tokenize_data method. This method takes the given data and generates a list of token IDs for a given review along with it's label. You would need to use the tokenizer to generate the token ids (hint:refer to tokenizer.encode_plus for more details) values for each review. **Please ensure that the maximum length of an encoded review is 512 tokens. If any input data is longer than 512 words/tokens, truncate it to first 512.** \n",
        "\n",
        "You would also need to convert the labels to a corresponding numerical class using the label_dict dictionary. Please write your code between the given TODO block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yJHPbHud-nH"
      },
      "outputs": [],
      "source": [
        "class DatasetLoader(Dataset):\n",
        "\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def tokenize_data(self):\n",
        "    print(\"Processing data..\")\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    label_dict = {'positive': 1, 'negative': 0}\n",
        "\n",
        "    review_list = self.data['review'].to_list()\n",
        "    label_list = self.data['sentiment'].to_list()\n",
        "\n",
        "    for (review, label) in tqdm(zip(review_list, label_list), total=len(review_list)):\n",
        "      # TODO(students): start\n",
        "      token_dict = self.tokenizer.encode_plus(review, max_length = 512)\n",
        "      input_ids = token_dict[\"input_ids\"]\n",
        "      tokens.append(torch.tensor(input_ids))\n",
        "      labels.append(torch.tensor(label_dict[label]))\n",
        "      # TODO(students): end\n",
        "    tokens = pad_sequence(tokens, batch_first=True)\n",
        "    labels = torch.tensor(labels)\n",
        "    dataset = TensorDataset(tokens, labels)\n",
        "    return dataset\n",
        "\n",
        "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
        "    processed_dataset = self.tokenize_data()\n",
        "\n",
        "    data_loader = DataLoader(\n",
        "        processed_dataset,\n",
        "        shuffle=shuffle,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIF3eaP3Lh0W"
      },
      "source": [
        "## **Problem 3 (Training Function)**\n",
        "---\n",
        "In this problem, you will write the code that will be used to run your model class on the dataset class, both of which you have written in the previous problems.\n",
        "\n",
        "The class below provides method to train a given model. It takes a dictionary with the following parameters:\n",
        "\n",
        "\n",
        "1.   device: The device to run the model on.\n",
        "2.   train_data: The train_data dataframe.\n",
        "3.   val_data: The val_data dataframe.\n",
        "4.   batch_size: The batch_size which is input to the model.\n",
        "5.   epochs: The number of epochs to train the model.\n",
        "6.   training_type: The type of training that your model will be undergoing. This can take four values - 'frozen_embeddings', 'top_2_training', 'top_4_training' and 'all_training'.\n",
        "\n",
        "#### **Problem 3(a)**\n",
        "\n",
        "Your first problem here would be to implement the set_training_parameters() method. In this method you will select the layers of your model to train based on the training_type. **Note: By default the Hugging Face DistilBERT has 6 layers.**\n",
        "\n",
        "1. frozen_embeddings: This setting is supposed to train the DistilBERT model with embeddings that are 'frozen' i.e., not trainable. You would need to ensure that the embedding layers in your model are not trainable.\n",
        "2. top_2_training: This setting is supposed to train just the final two layers of DistilBERT (layer 5 and layer 4). All other layers before these would need to be frozen.\n",
        "3. top_4_training: This setting is supposed to train just the final four layers of DistilBERT (layer 5, layer 4, layer 3 and layer 2). All other layers before these would need to be frozen.\n",
        "4. all_training: All layers of DistilBERT would need to trained.\n",
        "\n",
        "Please write your code between the given TODO block.\n",
        "\n",
        "**Note: The classifier head on top of the final DistilBERT layer would always need to be trained, please do not freeze that layer.**\n",
        "\n",
        "**Note: You can use model.named_parameters() and iterate over all the named parameters of the model. To set the layers to not be trainable, apply layer.requires_grad = false**\n",
        "\n",
        "#### **Problem 3(b)**\n",
        "\n",
        "Your second problem would be to implement a single training step in the given loop inside the train() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would also need to propagate the loss backwards to the model and update the given optimizer's parameters.\n",
        "\n",
        "Please write your code between the given TODO block.\n",
        "\n",
        "#### **Problem 3(c)**\n",
        "\n",
        "Your second problem would be to implement a single validation step in the given loop inside the eval() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would need to ensure that the loss is not propagated backwards.\n",
        "\n",
        "Please write your code between the given TODO block.\n",
        "\n",
        "**Note: Consult the pytorch demos by the TAs during class for Problem 3(b) and 3(c).** (https://colab.research.google.com/drive/1Nf_5z4_g09KqOy0km4fyG4Kj2bRcEcCK?usp=sharing) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llVpLV9kiUST"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "\n",
        "  def __init__(self, options):\n",
        "    self.device = options['device']\n",
        "    self.train_data = options['train_data']\n",
        "    self.val_data = options['val_data']\n",
        "    self.batch_size = options['batch_size']\n",
        "    self.epochs = options['epochs']\n",
        "    self.save_path = options['save_path']\n",
        "    self.training_type = options['training_type']\n",
        "    transformer = DistillBERT()\n",
        "    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
        "    self.model.to(self.device)\n",
        "\n",
        "  def get_performance_metrics(self, preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    precision = precision_score(labels_flat, pred_flat, zero_division=0)\n",
        "    recall = recall_score(labels_flat, pred_flat, zero_division=0)\n",
        "    f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n",
        "    return precision, recall, f1\n",
        "\n",
        "  def set_training_parameters(self):\n",
        "    # TODO(students): start\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if(self.training_type == \"top_2_training\"):\n",
        "        if \"pre_classifier\" in name:\n",
        "          continue\n",
        "        elif \"classifier\" in name:\n",
        "          continue\n",
        "        elif \"embeddings\" in name:\n",
        "          continue\n",
        "        if not(\"layer.4\" in name) or not(\"layer.5\" in name):\n",
        "          param.requires_grad = False\n",
        "\n",
        "      elif(self.training_type == \"top_4_training\"):\n",
        "        if \"layer.0\" in name:\n",
        "          param.requires_grad = False\n",
        "        elif \"layer.1\" in name:\n",
        "          param.requires_grad = False\n",
        "\n",
        "      elif(self.training_type == \"frozen_embeddings\"):\n",
        "        if \"embeddings\" in name:\n",
        "          param.requires_grad = False\n",
        "    \n",
        "    # TODO(students): end\n",
        "\n",
        "  def train(self, data_loader, optimizer):\n",
        "    self.model.train()\n",
        "    total_recall = 0\n",
        "    total_precision = 0\n",
        "    total_f1 = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (reviews, labels) in enumerate(tqdm(data_loader)):\n",
        "      self.model.zero_grad()\n",
        "      # TODO(students): start\n",
        "      reviews = reviews.to(self.device)\n",
        "      labels = labels.to(self.device)\n",
        "      outputs = self.model(reviews, labels = labels)\n",
        "      logits = outputs.logits\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      current_loss = outputs.loss\n",
        "      \n",
        "      total_loss += current_loss\n",
        "    \n",
        "      label2 = labels.to(\"cpu\").numpy()\n",
        "      current_precision, current_recall, current_f1 = self.get_performance_metrics(logits, label2)\n",
        "      total_precision += current_precision\n",
        "      total_recall += current_recall\n",
        "      total_f1 += current_f1\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      current_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # TODO(students): end\n",
        "\n",
        "    precision = total_precision/len(data_loader)\n",
        "    recall = total_recall/len(data_loader)\n",
        "    f1 = total_f1/len(data_loader)\n",
        "    loss = total_loss/len(data_loader)\n",
        "\n",
        "    return precision, recall, f1, loss\n",
        "\n",
        "  def eval(self, data_loader):\n",
        "    self.model.eval()\n",
        "    total_recall = 0\n",
        "    total_precision = 0\n",
        "    total_f1 = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for (reviews, labels) in tqdm(data_loader):\n",
        "        # TODO(students): start\n",
        "        reviews = reviews.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "        outputs = self.model(reviews, labels = labels)\n",
        "        logits = outputs.logits\n",
        "        current_loss = outputs.loss\n",
        "        total_loss += current_loss\n",
        "        \n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label2 = labels.to(\"cpu\").numpy()\n",
        "\n",
        "        current_precision, current_recall, current_f1 = self.get_performance_metrics(logits, label2)\n",
        "        total_precision += current_precision\n",
        "        total_recall += current_recall\n",
        "        total_f1 += current_f1\n",
        "        # TODO(students): end\n",
        "    \n",
        "    precision = total_precision/len(data_loader)\n",
        "    recall = total_recall/len(data_loader)\n",
        "    f1 = total_f1/len(data_loader)\n",
        "    loss = total_loss/len(data_loader)\n",
        "\n",
        "    return precision, recall, f1, loss\n",
        "\n",
        "  def save_transformer(self):\n",
        "    self.model.save_pretrained(self.save_path)\n",
        "    self.tokenizer.save_pretrained(self.save_path)\n",
        "\n",
        "  def execute(self):\n",
        "    last_best = 0\n",
        "    train_dataset = DatasetLoader(self.train_data, self.tokenizer)\n",
        "    train_data_loader = train_dataset.get_data_loaders(self.batch_size)\n",
        "    val_dataset = DatasetLoader(self.val_data, self.tokenizer)\n",
        "    val_data_loader = val_dataset.get_data_loaders(self.batch_size)\n",
        "    optimizer = AdamW(self.model.parameters(), lr = 3e-5, eps = 1e-8)\n",
        "    self.set_training_parameters()\n",
        "    for epoch_i in range(0, self.epochs):\n",
        "      train_precision, train_recall, train_f1, train_loss = self.train(train_data_loader, optimizer)\n",
        "      print(f'Epoch {epoch_i + 1}: train_loss: {train_loss:.4f} train_precision: {train_precision:.4f} train_recall: {train_recall:.4f} train_f1: {train_f1:.4f}')\n",
        "      val_precision, val_recall, val_f1, val_loss = self.eval(val_data_loader)\n",
        "      print(f'Epoch {epoch_i + 1}: val_loss: {val_loss:.4f} val_precision: {val_precision:.4f} val_recall: {val_recall:.4f} val_f1: {val_f1:.4f}')\n",
        "\n",
        "      if val_f1 > last_best:\n",
        "        print(\"Saving model..\")\n",
        "        self.save_transformer()\n",
        "        last_best = val_f1\n",
        "        print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bebt5gVsPSUi"
      },
      "source": [
        "**Notes: Run the following blocks in order to train the model and save it in your Google Drive. There will be variations due to random initializations. Most of the experiment validation and test accuracy should be between 80%-95%. Each experiment should not take more than 30 minutes to run when runtime is set to GPU.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt8LCF-CQRzS"
      },
      "source": [
        "#### **Experiment 1**\n",
        "---\n",
        "Training your DistilBERT with frozen embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWi6MQDJn3MO",
        "outputId": "195ba2c2-e8b1-4adc-ac02-d990a9f764cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5130 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 5130/5130 [00:04<00:00, 1034.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 270/270 [00:00<00:00, 941.45it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 321/321 [07:34<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss: 0.3516 train_precision: 0.8674 train_recall: 0.8643 train_f1: 0.8474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:08<00:00,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: val_loss: 0.2140 val_precision: 0.9392 val_recall: 0.9095 val_f1: 0.9186\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 321/321 [07:36<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss: 0.1933 train_precision: 0.9284 train_recall: 0.9268 train_f1: 0.9212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: val_loss: 0.2119 val_precision: 0.9179 val_recall: 0.9673 val_f1: 0.9389\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 321/321 [07:40<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss: 0.1113 train_precision: 0.9591 train_recall: 0.9639 train_f1: 0.9581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: val_loss: 0.2012 val_precision: 0.9266 val_recall: 0.9642 val_f1: 0.9426\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['train_data'] = train_data\n",
        "options['val_data'] = val_data\n",
        "options['save_path'] = SAVE_PATH + '_frozen_embeddings'\n",
        "options['epochs'] = EPOCHS\n",
        "options['training_type'] = 'frozen_embeddings'\n",
        "trainer = Trainer(options)\n",
        "trainer.execute()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ_PDuUAQcGb"
      },
      "source": [
        "#### **Experiment 2**\n",
        "---\n",
        "Training your DistilBERT with only top 2 layers being trained. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrZS-fPSaq7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3631220-418a-41bc-d031-daada6d862e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5130 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 5130/5130 [00:05<00:00, 983.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 270/270 [00:00<00:00, 989.12it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 321/321 [06:00<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss: 0.6486 train_precision: 0.6622 train_recall: 0.7758 train_f1: 0.6856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: val_loss: 0.5492 val_precision: 0.7591 val_recall: 0.8496 val_f1: 0.7895\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 321/321 [06:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss: 0.3975 train_precision: 0.8589 train_recall: 0.8524 train_f1: 0.8446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: val_loss: 0.3123 val_precision: 0.8770 val_recall: 0.8941 val_f1: 0.8765\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 321/321 [06:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss: 0.2246 train_precision: 0.9271 train_recall: 0.9165 train_f1: 0.9160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: val_loss: 0.2703 val_precision: 0.8678 val_recall: 0.9240 val_f1: 0.8886\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['train_data'] = train_data\n",
        "options['val_data'] = val_data\n",
        "options['save_path'] = SAVE_PATH + '_top_2_training'\n",
        "options['epochs'] = EPOCHS\n",
        "options['training_type'] = 'top_2_training'\n",
        "trainer = Trainer(options)\n",
        "trainer.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLlQtrpOQhr1"
      },
      "source": [
        "#### **Experiment 3**\n",
        "---\n",
        "Training your DistilBERT with only top 4 layers being trained. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMkzOXZja66z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35229e6e-3f7d-4a2a-d9d2-10bc933dcd67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5130 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 5130/5130 [00:04<00:00, 1027.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 270/270 [00:00<00:00, 1040.86it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 321/321 [07:14<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss: 0.4210 train_precision: 0.7769 train_recall: 0.7745 train_f1: 0.7482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: val_loss: 0.2790 val_precision: 0.8291 val_recall: 0.9829 val_f1: 0.8959\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 321/321 [07:14<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss: 0.1949 train_precision: 0.9256 train_recall: 0.9291 train_f1: 0.9202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: val_loss: 0.2085 val_precision: 0.9329 val_recall: 0.8921 val_f1: 0.9047\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 321/321 [07:14<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss: 0.1037 train_precision: 0.9609 train_recall: 0.9685 train_f1: 0.9617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: val_loss: 0.2031 val_precision: 0.9337 val_recall: 0.9423 val_f1: 0.9346\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['train_data'] = train_data\n",
        "options['val_data'] = val_data\n",
        "options['save_path'] = SAVE_PATH + '_top_4_training'\n",
        "options['epochs'] = EPOCHS\n",
        "options['training_type'] = 'top_4_training'\n",
        "trainer = Trainer(options)\n",
        "trainer.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMqn3uoTQkwV"
      },
      "source": [
        "#### **Experiment 4**\n",
        "---\n",
        "Training your DistilBERT with all layers being trained. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd1DUHuZa9Zz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c86fa0-b9a0-41c2-e7ce-9742bbe0f9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5130 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 5130/5130 [00:05<00:00, 1017.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 270/270 [00:00<00:00, 1017.50it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 321/321 [07:49<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss: 0.4801 train_precision: 0.7384 train_recall: 0.7480 train_f1: 0.7068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:09<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: val_loss: 0.2460 val_precision: 0.9167 val_recall: 0.8878 val_f1: 0.8974\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 321/321 [07:44<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss: 0.2014 train_precision: 0.9301 train_recall: 0.9219 train_f1: 0.9195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:08<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: val_loss: 0.2295 val_precision: 0.8808 val_recall: 0.9590 val_f1: 0.9137\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 321/321 [07:44<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss: 0.1004 train_precision: 0.9713 train_recall: 0.9690 train_f1: 0.9678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:08<00:00,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: val_loss: 0.2476 val_precision: 0.9207 val_recall: 0.9504 val_f1: 0.9320\n",
            "Saving model..\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['train_data'] = train_data\n",
        "options['val_data'] = val_data\n",
        "options['save_path'] = SAVE_PATH + '_all_training'\n",
        "options['epochs'] = EPOCHS\n",
        "options['training_type'] = 'all_training'\n",
        "trainer = Trainer(options)\n",
        "trainer.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JeEqQ1bQ4j3"
      },
      "source": [
        "## **Problem 4 (Test Function)**\n",
        "---\n",
        "Here, you will write the code for the testing of the models that you trained in the previous code blocks. \n",
        "\n",
        "The class below provides method to test a given model. It takes a dictionary with the following parameters:\n",
        "\n",
        "1.   device: The device to run the model on.\n",
        "2.   test_data: The test_data dataframe.\n",
        "3.   batch_size: The batch_size which is input to the model.\n",
        "4.   save_path: The directory of your saved model.\n",
        "\n",
        "You would need to implement a single test step in the given loop inside the test() method. You would need to pass the review and label in the given batch to the model, take the output and compute the Precision, Recall and F1 for that batch using the get_performance_metrics() method. You would need to ensure that the loss is not propagated backwards.\n",
        "\n",
        "Please write your code between the given TODO block.\n",
        "\n",
        "Hint: This problem is very similar to 3(c)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URDu5ew3q5ke"
      },
      "outputs": [],
      "source": [
        "class Tester():\n",
        "\n",
        "  def __init__(self, options):\n",
        "    self.save_path = options['save_path']\n",
        "    self.device = options['device']\n",
        "    self.test_data = options['test_data']\n",
        "    self.batch_size = options['batch_size']\n",
        "    transformer = DistillBERT(self.save_path)\n",
        "    self.model, self.tokenizer = transformer.get_tokenizer_and_model()\n",
        "    self.model.to(self.device)\n",
        "\n",
        "  def get_performance_metrics(self, preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    precision = precision_score(labels_flat, pred_flat, zero_division=0)\n",
        "    recall = recall_score(labels_flat, pred_flat, zero_division=0)\n",
        "    f1 = f1_score(labels_flat, pred_flat, zero_division=0)\n",
        "    return precision, recall, f1\n",
        "\n",
        "  def test(self, data_loader):\n",
        "    self.model.eval()\n",
        "    total_recall = 0\n",
        "    total_precision = 0\n",
        "    total_f1 = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for (reviews, labels) in tqdm(data_loader):\n",
        "        # TODO(students): start\n",
        "        reviews = reviews.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "        outputs = self.model(reviews, labels = labels)\n",
        "        logits = outputs.logits\n",
        "        current_loss = outputs.loss\n",
        "        total_loss += current_loss\n",
        "        \n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label2 = labels.to(\"cpu\").numpy()\n",
        "\n",
        "        current_precision, current_recall, current_f1 = self.get_performance_metrics(logits, label2)\n",
        "        total_precision += current_precision\n",
        "        total_recall += current_recall\n",
        "        total_f1 += current_f1\n",
        "\n",
        "        # TODO(students): end\n",
        "    \n",
        "    precision = total_precision/len(data_loader)\n",
        "    recall = total_recall/len(data_loader)\n",
        "    f1 = total_f1/len(data_loader)\n",
        "    loss = total_loss/len(data_loader)\n",
        "\n",
        "    return precision, recall, f1, loss\n",
        "\n",
        "  def execute(self):\n",
        "    test_dataset = DatasetLoader(self.test_data, self.tokenizer)\n",
        "    test_data_loader = test_dataset.get_data_loaders(self.batch_size)\n",
        "\n",
        "    test_precision, test_recall, test_f1, test_loss = self.test(test_data_loader)\n",
        "\n",
        "    print()\n",
        "    print(f'test_loss: {test_loss:.4f} test_precision: {test_precision:.4f} test_recall: {test_recall:.4f} test_f1: {test_f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1d4bih3SMTM"
      },
      "source": [
        "**Notes: Run these blocks only after Experiment 1 to 4 are completed and the models are saved in the \"models\" folder. Copy the output blocks into another text file for report writing.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkeN4URqRkRD"
      },
      "source": [
        "#### **Experiment 5**\n",
        "---\n",
        "Testing your DistilBERT trained with frozen embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiCG8IQl0qnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79cd6fba-6820-4ce5-b4c2-d27534b83b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/600 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 600/600 [00:00<00:00, 1007.77it/s]\n",
            "100%|██████████| 38/38 [00:19<00:00,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_loss: 0.2694 test_precision: 0.8759 test_recall: 0.8820 test_f1: 0.8683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['test_data'] = test_data\n",
        "options['save_path'] = SAVE_PATH + '_frozen_embeddings'\n",
        "tester = Tester(options)\n",
        "tester.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXy2Yv6sRo2F"
      },
      "source": [
        "#### **Experiment 6**\n",
        "---\n",
        "Testing your DistilBERT trained with all layers frozen except the final two layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y13wweAicsEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e7cbe9-0188-4083-b43e-7d0cb3b83606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/600 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 600/600 [00:00<00:00, 1020.61it/s]\n",
            "100%|██████████| 38/38 [00:19<00:00,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_loss: 0.3423 test_precision: 0.8264 test_recall: 0.8581 test_f1: 0.8312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['test_data'] = test_data\n",
        "options['save_path'] = SAVE_PATH + '_top_2_training'\n",
        "tester = Tester(options)\n",
        "tester.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1CWEB-R06K"
      },
      "source": [
        "#### **Experiment 7**\n",
        "---\n",
        "Testing your DistilBERT trained with all layers frozen except the final four layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvBdkdX4cvSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb94af1b-764f-46be-d09c-da02a57b7bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/600 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 600/600 [00:00<00:00, 990.00it/s] \n",
            "100%|██████████| 38/38 [00:19<00:00,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_loss: 0.3247 test_precision: 0.8634 test_recall: 0.8849 test_f1: 0.8636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['test_data'] = test_data\n",
        "options['save_path'] = SAVE_PATH + '_top_4_training'\n",
        "tester = Tester(options)\n",
        "tester.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79g8GwS4R3eB"
      },
      "source": [
        "#### **Experiment 8**\n",
        "---\n",
        "Testing your DistilBERT trained with all layers trainable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0wt64hdcyqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e88f1a-f7ae-40af-eb90-12c980746b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/600 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 600/600 [00:00<00:00, 1016.86it/s]\n",
            "100%|██████████| 38/38 [00:19<00:00,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_loss: 0.3479 test_precision: 0.8605 test_recall: 0.9112 test_f1: 0.8777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "options = {}\n",
        "options['batch_size'] = BATCH_SIZE\n",
        "options['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "options['test_data'] = test_data\n",
        "options['save_path'] = SAVE_PATH + '_all_training'\n",
        "tester = Tester(options)\n",
        "tester.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5UR_s7ySnb0"
      },
      "source": [
        "## **Submission guidelines**\n",
        "---\n",
        "You would need to submit the following files:\n",
        "\n",
        "\n",
        "1.   `NLP_HW3.ipynb` - This jupyter notebook.\n",
        "2.   `gdrive_link.txt` - Should contain a wgetable to a folder that contains your four DistilBERT models. Please make sure you provide the necessary permissions.\n",
        "3. `<SBUID>_Report.pdf` - A PDF report as detailed below.\n",
        "\n",
        "Your PDF report should contain the answers to the following questions. Use the outputs of the code blocks that were asked to save for this task:\n",
        "\n",
        "1.   Explanation of your code implementations for each TO-DO tasks.\n",
        "2.   A table containing the precision, recall and F1 scores of each DistilBERT model during testing (Experiments 5 to 8).\n",
        "3. An analysis explaining your understanding of the impact freezing/training different layers has on the model's performance.\n",
        "\n",
        "\n",
        "**Colab design credit**: TA Dhruv Verma"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_HW3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}